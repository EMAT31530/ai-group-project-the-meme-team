Sensors: 13 (3*position, 4*rotation, 3*velocity, 3*angular velocity)
Actuators: 3 (joystick, throttle - mapping extended to [0,1.1] followed by clamping)
DOF: Full DOF
Drag: 0.1
Angular Drag: 0.1

Reward Shaping:
Penalty for leaving environment = -10f
Penalty for missing target = -5f
Reward for hitting target = 5f

Hyperparameter testing: low epsilon (0.1)

Observations/ Experimental Results:
Agent does not seem to adjust trajectory as it falls to the ground. Relative success in reaching the target seems to depend on the proximity of the agent's starting point to the target location. No significant visible difference in agent velocity or angular rotation throughout landing.
Tensorboard results: cumulative reward increases with time, policy loss is small (as seen above), value loss & value estimate decrease to a constant